---
title: "Big Data Analytics"
subtitle: 'Lecture 5:<br>Cloud Computing'
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../../style/ioslides_unilu.css
    template: ../../style/nologo_template.html
logo: ../img/logo_unilu.png
bibliography: ../references/bigdata.bib
---



```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```

# Updates

## Goals for today:

- Know what 'cloud computing' is, and what it is used for.
- Know how to work with RStudio-Server on an *AWS EC2* instance.
- Know how to scale up an EC2 instance.
- Review: parallel processing.
- Get familiar with *AWS EMR*.

# Cloud Services for Big Data Analytics

## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;

```{r cpu2, echo=FALSE, out.width = "30%", fig.align='center', purl=FALSE}
include_graphics("../../img/03_cpu.jpg")
```

Computationally intense tasks: parallelization, using several CPU cores (nodes) in parallel.



## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;


```{r ram2, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../../img/03_ram.jpg")
```

Memory-intense tasks (data still fits into RAM): efficient memory allocation (`data.table`-package).


## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;
```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="RAM and Harddrive",fig.show='hold',fig.align='center'}
include_graphics(c("../../img/03_ram.jpg", "../../img/03_harddrive.jpg"))
```

Memory-intense tasks (data does not fit into RAM): efficient use of virtual memory (use parts of mass storage device as virtual memory).

## Wrap-up: efficient use of CPU, RAM, Mass Storage

```{r harddrive3, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../../img/03_harddrive.jpg")
```

(Big) Data storage: efficient storage (avoid redundancies) and efficient access (speed) with RDBMSs (here: SQLite).


## Already using all components most efficiently?

- *Scale up ('vertical scaling')*
- *Scale out ('horizontal scaling')*

## 'The Cloud'


```{r cloud, echo=FALSE, out.width = "70%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_cloud.png")
```





## The Cloud: Scaling Up


```{r scaleup, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```


## The Cloud: Scaling Up


```{r scaleup2, echo=FALSE, out.width = "50%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```


## The Cloud: Scaling Up


```{r scaleup3, echo=FALSE, out.width = "50%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```

- Parallel computing, large in-memory computation, SQL/NoSQL databases, etc.
- Common in scientific computing.




## The Cloud: Scaling Out


```{r scaleout4, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```


```{r scaleout5, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```


```{r scaleout6, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../../img/07_server.png")
```

 - *Distributed Systems*: MapReduce/Hadoop etc.
 - Rather rare in an applied econometrics setting.


## The Cloud in Practice

Rent (virtual) machines on a flexible basis (hourly rate, etc.) from a cloud computing provider.

 - [Amazon Web Services (AWS)](https://aws.amazon.com/)
 - [Microsoft Azure](https://azure.microsoft.com/en-us/)
 - [Google Cloud Platform](https://cloud.google.com/)
 - [IBM Cloud](https://www.ibm.com/cloud/)
 - [Alibaba Cloud（阿里云)](https://www.alibabacloud.com/)
 - [Tencent Cloud (腾讯云)](https://intl.cloud.tencent.com/)
 - ...




# Scaling up with AWS EC2 and R/RStudio

## Set up AWS EC2 for R/RStudio

How to use [Louis Aslett's Amazon Machine Image (AMI)](https://www.louisaslett.com/RStudio_AMI/), to run RStudio-Server on an EC2 Instance.

- Depending on the region in which you want to initiate your EC2 instance, click on the corresponding AMI link in https://www.louisaslett.com/RStudio_AMI/ (e.g., [ami-076abd591c4335092](https://console.aws.amazon.com/ec2/home?region=eu-central-1#launchAmi=ami-076abd591c4335092) for Frankfurt) .

- Select the instance type, click on "Review and Launch". Select "Edit security groups" and switch the `SSH` entry to `HTTP`. Click again on "Review and Launch".

- Click "Launch",  select "Proceed without a key pair" from the drop-down menu and check the box below "I acknowledge ...". Click "Launch" to confirm. Click on "View" instances, wait until "Status check" is "2/2 checks passed".

- Click on the instance ID of your newly launched instance and copy the public IPv4 address, open a new browser window/tab, type in `http://`,  paste the IP address, and hit enter.

- Log into RStudio-Server with username `rstudio` and the instance ID as password.



## Parallelization with an EC2 instance

- You can install packages as usual with `install.packages`.
- Installations might take a bit longer (are using R/RStudio on a Linux machine, some packages need to be compiled).
- If you are running a free tier T2.micro instance, there will be only one core available. However, we still can easily test the parallelization code on this instance.

## Preparatory steps

- Open the AWS console in a browser window, make sure your EC2 instance is running.
- Open a new browser window, and connect to the instance, log in to RStudio-Server.
- Install the packages needed for this short tutorial

```{r, eval=FALSE}
# install packages for parallelization
install.packages("parallel", "doSNOW", "stringr")
```

## Prepare session


```{r eval=FALSE}
# load packages
library(parallel)
library(doSNOW)

# verify no. of cores available
n_cores <- detectCores()
n_cores
```

## Upload data

```{r ec2rstudioserver, echo=FALSE, out.width = "70%", fig.align='center', fig.cap= "File explorer and Upload-button on Rstudio-Server.", purl=FALSE}
include_graphics("../../img/screenshot_rstudio_server_upload.png")
```

## Save and run preparatory R-Script on EC2


```{r eval=FALSE}
# PREPARATION -----------------------------

# packages
library(stringr)

# import data
marketing <- read.csv("data/marketing_data.csv")
# clean/prepare data
marketing$Income <- as.numeric(gsub("[[:punct:]]", "", marketing$Income))
marketing$days_customer <- as.Date(Sys.Date())- as.Date(marketing$Dt_Customer, "%m/%d/%y")
marketing$Dt_Customer <- NULL

# all sets of independent vars
indep <- names(marketing)[ c(2:19, 27,28)]
combinations_list <- lapply(1:length(indep),
                            function(x) combn(indep, x, simplify = FALSE))
combinations_list <- unlist(combinations_list, recursive = FALSE)
models <- lapply(combinations_list,
                 function(x) paste("Response ~", paste(x, collapse="+")))
```


## Test parallelized code

- Test the code (with a small N) on EC2 without registering the one core for cluster processing.
- `%dopart%` will automatically resort to running the code sequentially.  

```{r eval=FALSE}
# set cores for parallel processing
# ctemp <- makeCluster(ncores)
# registerDoSNOW(ctemp)

# prepare loop
N <- 10 # just for illustration, the actual code is N <- length(models)
# run loop in parallel
pseudo_Rsq <-
  foreach ( i = 1:N, .combine = c) %dopar% {
    # fit the logit model via maximum likelihood
    fit <- glm(models[[i]], data=marketing, family = binomial())
    # compute the proportion of deviance explained by the independent vars (~R^2)
    return(1-(fit$deviance/fit$null.deviance))
}
```




## Scale up

- In the AWS console, stop the instance: "Instance state/stop instance".
- Select "Actions/Instance settings/change instance type" and choose a new instance type (e.g., `t2.2xlarge`, note that this is not free tier!)
- Start the instance again, log in to RStudio-Server.


## Run the script in parallel

```{r eval=FALSE}

# set cores for parallel processing
ctemp <- makeCluster(ncores)
registerDoSNOW(ctemp)

```


## Monitor processes/resources


```{r ec2rstudioserverhtop, echo=FALSE, out.width = "70%", fig.align='center', fig.cap= "Monitoring of resources with htop (in terminal).", purl=FALSE}
include_graphics("../../img/ec2_rstudioserver_htop.png")
```

# EC2 with RStudio and GPUs

## EC2 with RStudio and GPUs

To start a ready-made EC2 instance with GPUs and RStudio installed, go to this service provided by RStudio on the AWS Marketplace: https://aws.amazon.com/marketplace/pp/B0785SXYB2. Click on Subscribe.


```{r ec2gpusetup1, echo=FALSE, out.width = "70%", fig.align='center', fig.cap= "AWS Marketplace product provided by RStudio to run RStudio Server with Tensorflow-GPU on AWS EC2.", purl=FALSE}
include_graphics("../../img/ec2_gpu1.png")
```

## EC2 with RStudio and GPUs


- Click on Continue to Configuration and Continue to Launch. 
- Select under "EC2 Instance Type" `g3s.xlarge`. 
- If necessary, create a new key pair under 'Key Pair Settings', otherwise keep all the default settings as they are. 
- Click on *Launch*.

## EC2 with RStudio and GPUs


```{r ec2gpusetup2, echo=FALSE, out.width = "60%", fig.align='center', fig.cap= "Launch RStudio Server with Tensorflow-GPU on AWS EC2.", purl=FALSE}
include_graphics("../../img/ec2_gpu2.png")
```


## Use the GPU instance

- Open a new browser window and go to `http://<ec2_instance_public_dns>:8787`. 
- Log in to RStudio-Server with username `rstudio-user` and the instance ID of your newly created instance as the password.

## GPUs and TPUs on Google Colab



## GPUs and TPUs on Google Colab

1. Open a new browser window, go to https://colab.to/r and log in with your Google account if prompted to do so.

2. Click on "Runtime/Change runtime type" and select in the drop-down menu under 'Hardware accelerator' the option 'GPU' (or 'TPU').

```{r colabr, echo=FALSE, out.width = "50%", fig.align='center', fig.cap= "Colab notebook with R runtime and GPUs.", purl=FALSE}
include_graphics("../../img/colab_r_gpu.png")
```



## GPUs and TPUs on Google Colab

- Install the packages you need to work with GPU acceleration (e.g., `gpuR`, `keras` and `tensorflow`). 
- Example: [simply image classification tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_classification/) with keras on TPUs: [bit.ly/bda_colab](https://bit.ly/bda_colab).





# AWS EMR: MapReduce in the cloud

## AWS EMR: MapReduce in the cloud

- Easy-to-use solution provided by AWS, called *Elastic MapReduce (AWS EMR)*.
- Idea: connect several EC2 instances to a cluster and run Hadoop/Spark on it (via R/RStudio)



## Set up an EMR cluster to run with R

Preparatory step: [install the aws cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).

```{bash eval=FALSE}
aws emr create-cluster \
--release-label emr-6.1.0 \
--applications Name=Hadoop Name=Spark Name=Hive Name=Pig Name=Tez Name=Ganglia   \
--name "EMR 6.1 RStudio + sparklyr"  \
--service-role EMR_DefaultRole \
--instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5a.2xlarge \
InstanceGroupType=CORE,InstanceCount=4,InstanceType=m5a.2xlarge \
--bootstrap-action Path='s3://aws-bigdata-blog/artifacts/aws-blog-emr-rstudio-sparklyr/rstudio_sparklyr_emr6.sh',Name="Install RStudio" --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole,KeyName="sparklyr" \
--configurations '[{"Classification":"spark","Properties":{"maximizeResourceAllocation":"true"}}]' \
--region us-east-1
```


## Set up an EMR cluster to run with R

- Note: Setting up this cluster with all the additional software and configurations from the bootstrap script will take around 40 minutes. Once the cluster is ready, you will see something like this:


```{r emrsetup, echo=FALSE, out.width = "80%", fig.align='center', fig.cap= "AWS EMR console indicating the successful set up of the EMR cluster", purl=FALSE}
include_graphics("../../img/aws_emr_ready.png")
```

## Access RStudio on the cluster's master node

- Follow the prerequisites to connect to EMR via SSH: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-ssh-prereqs.html.

- Then initiate the SSH tunnel to the EMR cluster as instructed here: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html. 

- Protect your key-file (`sparklyr.pem`) by navigating to the location of the key-file on your computer in the terminal and run  `chmod 600 sparklyr.pem` before connecting. Also make sure your IP address is still the one you have entered in the previous step (you can check your current IP address by visiting https://whatismyipaddress.com/).

- In the terminal, connect to the EMR cluster via SSH by running `ssh -i ~/sparklyr.pem -ND 8157 hadoop@ec2-52-87-248-175.compute-1.amazonaws.com` (if you have protected the key-file as super user, i.e. `sudo chmod`, you will need to use `sudo ssh` here).

- In your Firefox browser, install the [FoxyProxy add on](https://addons.mozilla.org/en-US/firefox/addon/foxyproxy-standard/). Follow these instructions to set up the proxy via FoxyProxy: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-proxy.html.
- Select the newly created Socks5 proxy in FoxyProxy.
- Go to http://localhost:8787/ and enter with username `hadoop` and password `hadoop`. 






<!-- ## Regression analysis -->

<!-- - Correlation study of what factors are associated with more or less arrival delay. -->
<!-- - Built-in 'MLib' library several high-level functions for regression analyses. -->


<!-- ## Regression analysis: comparison with native R -->

<!-- - First estimate a linear model with the usual R approach (all computed in the R environment). -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- # flights_r <- collect(flights) # very slow! -->
<!-- flights_r <- data.table::fread("../data/flights.csv", nrows = 300) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # specify the linear model -->
<!-- model1 <- arr_delay ~ dep_delay + distance -->
<!-- # fit the model with ols -->
<!-- fit1 <- lm(model1, flights_r) -->
<!-- # compute t-tests etc. -->
<!-- summary(fit1) -->
<!-- ``` -->


<!-- ## Regression analysis: comparison with native R -->

<!-- - Compute essentially the same model estimate in `sparklyr`. -->
<!-- - Note that most regression models commonly used in traditional applied econometrics are also provided in `sparklyr` or `SparkR`. -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(sparklyr) -->

<!-- # connect with default configuration -->
<!-- sc <- spark_connect(master = "local", -->
<!--                     version = "2.4.5") -->

<!-- # load data to spark -->
<!-- flights2 <- copy_to(sc, flights_r, "flights2") -->

<!-- # fit the model -->
<!-- fit1_spark <- ml_linear_regression(flights2, formula = model1) -->
<!-- # compute t-tests etc. -->
<!-- summary(fit1_spark) -->
<!-- ``` -->



<!-- ## Regression analysis: comparison with native R -->

<!-- Alternative with `SparkR`: -->


<!-- ```{r message=FALSE, warning=FALSE, eval=FALSE} -->
<!-- # create SparkDataFrame -->
<!-- flights3 <- createDataFrame(flights_r) -->
<!-- # fit the model -->
<!-- fit2_spark <- spark.glm(formula = model1, data = flights3 , family="gaussian") -->
<!-- # compute t-tests etc. -->
<!-- summary(fit2_spark) -->

<!-- ``` -->




<!-- # GPUs for Scientific Computing -->

<!-- ## GPUs for scientific computing -->

<!-- - *Graphic Processing Units (GPUs)*. -->
<!-- - 'Side product' of the computer games industry. -->
<!-- - More demanding games needed better graphic cards (with faster *GPUs*). -->

<!-- ## GPUs -->

<!-- ```{r nvidiagpu, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE} -->
<!-- include_graphics("../img/nvidia_geeforce.png") -->
<!-- ``` -->

<!-- ## Why GPUs? -->

<!-- - Why not more powerful CPUs to deal with the more demanding PC games? -->
<!-- - CPUs: designed not only for *efficiency but also flexibility*. -->
<!-- - GPUs: designed to excel at computing graphics. -->
<!--   - Highly parallel numerical floating point workloads. -->
<!--   - Very useful in some core scientific computing tasks (see @fatahalian_etal2004)! -->

<!-- ## GPU characteristics -->

<!-- - Composed of several multiprocessor units. -->
<!-- - Each multiprocessor units has several cores. -->
<!-- - GPUs can perform computations with thousands of threads in parallel. -->

<!-- ## GPU characteristics -->


<!-- ```{r nvidia_architecture, echo=FALSE, out.width = "40%", fig.align='center', fig.cap= "Typical NVIDIA GPU architecture (illustration and notes by @hernandez_etal2013): The GPU is comprised of a set of Streaming MultiProcessors (SM). Each SM is comprised of several Stream Processor (SP) cores, as shown for the NVIDIA’s Fermi architecture (a). The GPU resources are controlled by the programmer through the CUDA programming model, shown in (b).", purl=FALSE} -->
<!-- include_graphics("../img/nvidia_gpu.png") -->
<!-- ``` -->

<!-- ## Challenges to using GPUs for scientific computing -->

<!-- - Different hardware architecture, different low-level programming model. -->
<!-- - Good understanding of hardware needed. -->
<!-- - But, more and more high-level APIs available (e.g., in tensorflow/keras). -->


<!-- # GPUs in R -->




<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->

<!-- - `gpuR`: basic R functions to compute with GPUs from within the R environment.  -->
<!-- - Example: compare the performance of the CPU with the GPU based on a matrix multiplication exercise.  -->
<!--   - (For a large $N\times P$ matrix $X$, we want to compute $X^tX$.) -->


<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->

<!-- ```{r  warning=FALSE, message=FALSE } -->
<!-- # load package -->
<!-- library(bench) -->
<!-- library(gpuR) -->

<!-- ``` -->

<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->

<!-- Initiate a large matrix filled with pseudo random numbers ($N$ observations and $P$ variables). -->

<!-- ```{r} -->
<!-- # initiate dataset with pseudo random numbers -->
<!-- N <- 10000  # number of observations -->
<!-- P <- 100 # number of variables -->
<!-- X <- matrix(rnorm(N * P, 0, 1), nrow = N, ncol =P) -->

<!-- ``` -->


<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->

<!-- Prepare for GPU computation. -->

<!-- ```{r} -->
<!-- # prepare GPU-specific objects/settings -->
<!-- gpuX <- gpuMatrix(X, type = "float")  # point GPU to matrix (matrix stored in non-GPU memory) -->
<!-- vclX <- vclMatrix(X, type = "float")  # transfer matrix to GPU (matrix stored in GPU memory) -->
<!-- ``` -->

<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->


<!-- Now we run the three examples: 1) using the CPU, 2) computing on the GPU but using CPU memory, 3) computing on the GPU and using GPU memory.  -->

<!-- ```{r message=FALSE} -->
<!-- # compare three approaches -->
<!-- gpu_cpu <- bench::mark( -->

<!--   # compute with CPU  -->
<!--   cpu <- t(X) %*% X, -->

<!--   # GPU version, GPU pointer to CPU memory (gpuMatrix is simply a pointer) -->
<!--   gpu1_pointer <- t(gpuX) %*% gpuX, -->

<!--   # GPU version, in GPU memory (vclMatrix formation is a memory transfer) -->
<!--   gpu2_memory <- t(vclX) %*% vclX, -->

<!-- check = FALSE, memory=FALSE, min_iterations = 20) -->
<!-- ``` -->


<!-- ## Example I: Matrix multiplication comparison (`gpuR`) -->


<!-- ```{r} -->
<!-- plot(gpu_cpu, type = "boxplot") -->
<!-- ``` -->






<!-- ##  Tensorflow/Keras example: predict housing prices -->

<!-- In this example we train a simple sequential model with two hidden layers in order to predict the median value of owner-occupied homes (in USD 1,000) in the Boston area (data are from the 1970s). The original data and a detailed description can be found [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The example follows closely [this keras tutorial](https://keras.rstudio.com/articles/tutorial_basic_regression.html#the-boston-housing-prices-dataset) published by RStudio. -->


<!-- ##  Tensorflow/Keras example: predict housing prices -->

<!-- ```{r echo=FALSE, message=FALSE, warning=FALSE} -->
<!-- if (Sys.info()["sysname"]=="Darwin"){ # run on mac os machine -->

<!--         use_python("/Users/umatter/opt/anaconda3/bin/python") # IMPORTANT: keras/tensorflow is set up to run in this environment on this machine! -->
<!-- } -->

<!-- ``` -->

<!-- ```{r warning=FALSE} -->
<!-- # load packages -->
<!-- library(keras) -->
<!-- library(tibble) -->
<!-- library(ggplot2) -->
<!-- library(tfdatasets) -->


<!-- # load data -->
<!-- boston_housing <- dataset_boston_housing() -->
<!-- str(boston_housing) -->
<!-- ``` -->


<!-- ##  Training and test dataset -->

<!-- - First split the data into a training set and a test set. -->
<!-- - Reason: Monitor the out-of-sample performance of the trained model! -->
<!--   - (Deep) neural nets are often susceptible to over-fitting. -->
<!--   - Validity checks based on the test sample are often an integral part of modelling with tensorflow/keras. -->

<!-- ```{r} -->
<!-- # assign training and test data/labels -->
<!-- c(train_data, train_labels) %<-% boston_housing$train -->
<!-- c(test_data, test_labels) %<-% boston_housing$test -->

<!-- ``` -->


<!-- ##  Prepare training data -->

<!-- In order to better understand and interpret the dataset we add the original variable names, and convert the dataset to a `tibble`.  -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- library(dplyr) -->

<!-- column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',  -->
<!--                   'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT') -->

<!-- train_df <- train_data %>%  -->
<!--   as_tibble(.name_repair = "minimal") %>%  -->
<!--   setNames(column_names) %>%  -->
<!--   mutate(label = train_labels) -->

<!-- test_df <- test_data %>%  -->
<!--   as_tibble(.name_repair = "minimal") %>%  -->
<!--   setNames(column_names) %>%  -->
<!--   mutate(label = test_labels) -->
<!-- ``` -->


<!-- ##  Inspect training data -->

<!-- Next, we have a close look at the data. Note the usage of the term 'label' for what is usually called the 'dependent variable' in econometrics. -->

<!-- <!-- ^[Typical textbook examples in machine learning deal with classification (e.g. a logit model), while in microeconometrics the typical example is usually a linear model (continuous dependent variable).] As the aim of the exercise is to predict median prices of homes, the output of the model will be a continuous value ('labels'). -->



<!-- ```{r} -->
<!-- # check example data dimensions and content -->
<!-- paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels)) -->
<!-- summary(train_data) -->
<!-- summary(train_labels) # Display first 10 entries -->
<!-- ``` -->

<!-- ##  Normalize features -->

<!-- - The dataset contains variables ranging from per capita crime rate to indicators for highway access (different units, different scales). -->
<!-- - Not per se a problem, but fitting is more efficient when all features are normalized. -->


<!-- ## Normalize features -->


<!-- ```{r} -->
<!-- spec <- feature_spec(train_df, label ~ . ) %>%  -->
<!--   step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%  -->
<!--   fit() -->

<!-- layer <- layer_dense_features( -->
<!--   feature_columns = dense_features(spec),  -->
<!--   dtype = tf$float32 -->
<!-- ) -->
<!-- layer(train_df) -->

<!-- ``` -->

<!-- ## Model specification -->

<!-- We specify the model as a linear stack of layers:  -->

<!--  - The input (all 13 explanatory variables). -->
<!--  - Two densely connected hidden layers (each with a 64-dimensional output space). -->
<!--  - The one-dimensional output layer (the 'dependent variable'). -->


<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- # Create the model -->
<!-- # model specification -->
<!-- input <- layer_input_from_dataset(train_df %>% select(-label)) -->

<!-- output <- input %>%  -->
<!--   layer_dense_features(dense_features(spec)) %>%  -->
<!--   layer_dense(units = 64, activation = "relu") %>% -->
<!--   layer_dense(units = 64, activation = "relu") %>% -->
<!--   layer_dense(units = 1)  -->

<!-- model <- keras_model(input, output) -->

<!-- ``` -->

<!-- ## Training configuration -->

<!-- In order to fit the model, we first have to 'compile' it (configure it for training): -->

<!-- - Set the parameters that will guide the training/optimization procedure.  -->
<!--     - Mean squared errors loss function (`mse`) typically used for regressions.  -->
<!--     - [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) optimizer to find the minimum loss. -->

<!-- ```{r} -->
<!-- # compile the model   -->
<!-- model %>%  -->
<!--   compile( -->
<!--     loss = "mse", -->
<!--     optimizer = optimizer_rmsprop(), -->
<!--     metrics = list("mean_absolute_error") -->
<!--   ) -->
<!-- ``` -->

<!-- ## Training configuration -->

<!-- Now we can get a summary of the model we are about to fit to the data. -->

<!-- ```{r} -->
<!-- # get a summary of the model -->
<!-- model -->
<!-- ``` -->

<!-- ## Monitoring training progress -->

<!-- - Set number of epochs. -->

<!-- ```{r} -->

<!-- # Set max. number of epochs -->
<!-- epochs <- 500 -->

<!-- ``` -->


<!-- ## Fit (train) the model -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- # Fit the model and store training stats -->

<!-- history <- model %>% fit( -->
<!--   x = train_df %>% select(-label), -->
<!--   y = train_df$label, -->
<!--   epochs = epochs, -->
<!--   validation_split = 0.2, -->
<!--   verbose = 0 -->
<!-- ) -->


<!-- plot(history) -->
<!-- ``` -->

<!-- ## Parallelization: A word of caution -->

<!-- Why not always use GPUs for parallel tasks in scientific computing?  -->

<!-- - Whether a GPU implementation is faster, depends on many factors. -->
<!-- - Also, proper implementation of parallel tasks (either on GPUs or CPUs) can be very tricky (and a lot of work). -->


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>
