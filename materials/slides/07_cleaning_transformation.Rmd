---
title: "Big Data Analytics"
subtitle: 'Lecture 7:<br>Cleaning and Transformation of Big Data'
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../../style/ioslides.css
    template: ../../style/nologo_template.html
logo: ../../img/logo.png
bibliography: ../references/bigdata.bib
---



```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```




 
# Recap 

## Recap: Database, Datawarehouse, Datalake

- Local RDBMS (e.g. SQLite) for data analytics projects: efficient/fast access to data, hardly any memory-intense operations for filtering, joining, selecting data.
- Flexible (also short-term) data storage solutions in the cloud.
- AWS RDS, Google BigQuery, AWS S3


## Goals for today

1. Know basic strategies for out-of-memory operations in R.
2. Know basic tools for local big data cleaning and transformation in R.
3. Understand (in simple terms) how these tools work.
4. Apply these tools for data cleaning and transformation.
4. (Recap of virtual memory concept)

# Virtual Memory

## Virtual memory review

- Operating system allocates part of mass storage device (hard-disk) as *virtual memory*. 
- Process/application uses up too much RAM, OS starts *swapping* data between RAM and virtual memory.
- Processes slow down due to swapping.
- Default (OS) usage of virtual memory concept is not necessarily optimized for data analysis tasks.

## Virtual memory review


```{r vm1, echo=FALSE, out.width = "30%", fig.align='center', purl=FALSE}
include_graphics("../../img/03_virtualmemory_dark.png")
```


## Virtual memory: example (linux)

```{r vm2, echo=FALSE, out.width = "95%", fig.align='center', purl=FALSE}
include_graphics("../../img/05_virtual_memory_linux.gif")
```

## 'Out-of-memory' strategies

- Use virtual memory idea for specific data analytics tasks.
- Two approaches:
  - *Chunked data files on disk*: partition large data set, map and store chunks of raw data on disk. Keep mapping in RAM. (`ff`-package)
  - *Memory mapped files and shared memory*: virtual memory is explicitly allocated for one or several specific data analytics tasks (different processes can access the same memory segment). (`bigmemory`-package)


## Chunking data with the `ff`-package

Preparations 
```{r message=FALSE, warning=FALSE}

# SET UP --------------

# install.packages(c("ff", "ffbase"))
# load packages
library(ff)
library(ffbase)
library(pryr)

# create directory for ff chunks, and assign directory to ff 
system("mkdir ffdf")
options(fftempdir = "ffdf")

```


## Chunking data with the `ff`-package

Import data, inspect change in RAM.

```{r echo=FALSE, message=FALSE, warning=FALSE}
gc()
```


```{r message=FALSE, warning=FALSE}
mem_change(
flights <- 
     read.table.ffdf(file="../../data/flights.csv",
                     sep=",",
                     VERBOSE=TRUE,
                     header=TRUE,
                     next.rows=100000,
                     colClasses=NA)
)
```


## Chunking data with the `ff`-package

Inspect file chunks on disk and data structure in R environment.

```{r message=FALSE, warning=FALSE}
# show the files in the directory keeping the chunks
list.files("ffdf")

# investigate the structure of the object created in the R environment
summary(flights)
```



## Memory mapping with `bigmemory`

Preparations

```{r message=FALSE, warning=FALSE}

# SET UP ----------------

# load packages
library(bigmemory)
library(biganalytics)
```



## Memory mapping with `bigmemory`

Import data, inspect change in RAM.

```{r message=FALSE, warning=FALSE}
# import the data
flights <- read.big.matrix("../../data/flights.csv",
                     type="integer",
                     header=TRUE,
                     backingfile="flights.bin",
                     descriptorfile="flights.desc")
```


## Memory mapping with `bigmemory`

Inspect the imported data.

```{r message=FALSE, warning=FALSE}
summary(flights)
```


## Memory mapping with `bigmemory`

Inspect the object loaded into the R environment.

```{r message=FALSE, warning=FALSE}
flights
```


## Memory mapping with `bigmemory`

- `backingfile`: The cache for the imported file (holds the raw data on disk).
- `descriptorfile`: Metadata describing the imported data set (also on disk).


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

First, import a large data set without a backing-file:

```{r message=FALSE, warning=FALSE}
# import data and check time needed  
system.time(
     flights1 <- read.big.matrix("../../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

# import data and check memory used
mem_change(
     flights1 <- read.big.matrix("../../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

flights1 
```




## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Second, import the same data set with a backing-file:

```{r message=FALSE, warning=FALSE}
# import data and check time needed  
system.time(
     flights2 <- read.big.matrix("../../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

# import data and check memory used
mem_change(
     flights2 <- read.big.matrix("../../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

flights2
```


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Third, re-import the same data set with a backing-file.

```{r message=FALSE, warning=FALSE}
# remove the loaded file
rm(flights2)

# 'load' it via the backing-file
system.time(flights2 <- attach.big.matrix("flights2.desc"))

flights2

```



# Cleaning and Transformation 

---

```{r pipeline, echo=FALSE, out.width = "95%", fig.align='center', purl=FALSE}
include_graphics("../../img/05_data_science_pipeline.png")
```



## Typical tasks (independent of data set size)

- Normalize/standardize.
- Code additional variables (indicators, strings to categorical, etc.).
- Remove, add covariates.
- Merge data sets.
- Set data types.

## Typical workflow

1. Import raw data.
2. Clean/transform.
3. Store for analysis.
     - Write to file.
     - Write to database.
     
## Bottlenecks

- RAM:
     - Raw data does not fit into memory.
     - Transformations enlarge RAM allocation (copying).
- Mass Storage: Reading/Writing
- CPU: Parsing (data types)



# Data Preparation with `ff`

## Set up

The following examples are based on @walkowiak_2016, Chapter 3.

```{r warning=FALSE, message=FALSE}

## SET UP ------------------------

#Set working directory to the data and airline_id files.
# setwd("materials/code_book/B05396_Ch03_Code")
system("mkdir ffdf")
options(fftempdir = "ffdf")

# load packages
library(ff)
library(ffbase)
library(pryr)

# fix vars
FLIGHTS_DATA <- "../../data/flights_sep_oct15.txt"
AIRLINES_DATA <- "../../data/airline_id.csv"

```

## Data import

```{r message=FALSE, warning=FALSE}

# DATA IMPORT ------------------

# 1. Upload flights_sep_oct15.txt and airline_id.csv files from flat files. 

system.time(flights.ff <- read.table.ffdf(file=FLIGHTS_DATA,
                                          sep=",",
                                          VERBOSE=TRUE,
                                          header=TRUE,
                                          next.rows=100000,
                                          colClasses=NA))

airlines.ff <- read.csv.ffdf(file= AIRLINES_DATA,
                             VERBOSE=TRUE,
                             header=TRUE,
                             next.rows=100000,
                             colClasses=NA)
# check memory used
mem_used()

```


## Comparison with `read.table`

```{r message=FALSE, warning=FALSE}

##Using read.table()
system.time(flights.table <- read.table(FLIGHTS_DATA, 
                                        sep=",",
                                        header=TRUE))

gc()

system.time(airlines.table <- read.csv(AIRLINES_DATA,
                                       header = TRUE))


# check memory used
mem_used()

```


## Inspect imported files

```{r message=FALSE, warning=FALSE}
# 2. Inspect the ffdf objects.
## For flights.ff object:
class(flights.ff)
dim(flights.ff)
## For airlines.ff object:
class(airlines.ff)
dim(airlines.ff)

```

## Data cleaning and transformation

Goal: merge airline data to flights data


```{r message=FALSE, warning=FALSE}
# step 1: 
## Rename "Code" variable from airlines.ff to "AIRLINE_ID" and "Description" into "AIRLINE_NM".
names(airlines.ff) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.ff)
str(airlines.ff[1:20,])
```


## Data cleaning and transformation

Goal: merge airline data to flights data

```{r message=FALSE, warning=FALSE}
# merge of ffdf objects
mem_change(flights.data.ff <- merge.ffdf(flights.ff, airlines.ff, by="AIRLINE_ID"))

class(flights.data.ff)
dim(flights.data.ff)
dimnames(flights.data.ff)
```

## Inspect difference to in-memory operation

```{r message=FALSE, warning=FALSE}
##For flights.table:
names(airlines.table) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.table)
str(airlines.table[1:20,])

# check memory usage of merge in RAM 
mem_change(flights.data.table <- merge(flights.table,
                                       airlines.table,
                                       by="AIRLINE_ID"))
```



## Subsetting

```{r message=FALSE, warning=FALSE}
mem_used()

# Subset the ffdf object flights.data.ff:
subs1.ff <- subset.ffdf(flights.data.ff, CANCELLED == 1, 
                        select = c(FL_DATE, AIRLINE_ID, 
                                   ORIGIN_CITY_NAME,
                                   ORIGIN_STATE_NM,
                                   DEST_CITY_NAME,
                                   DEST_STATE_NM,
                                   CANCELLATION_CODE))

dim(subs1.ff)
mem_used()

```


## Save to ffdf-files
(For further processing with `ff`)

```{r message=FALSE, warning=FALSE}
# Save a newly created ffdf object to a data file:

save.ffdf(subs1.ff, overwrite = TRUE) #7 files (one for each column) created in the ffdb directory

```


## Load ffdf-files

```{r message=FALSE, warning=FALSE}
# Loading previously saved ffdf files:
rm(subs1.ff)
gc()
load.ffdf("ffdb")
str(subs1.ff)
dim(subs1.ff)
dimnames(subs1.ff)
```

## Export to CSV

```{r message=FALSE, warning=FALSE}
#  Export subs1.ff into CSV and TXT files:
write.csv.ffdf(subs1.ff, "subset1.csv")

```




## References

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>