---
title: "Big Data Analytics"
subtitle: 'Lecture 2: Computation and Memory'
author: "Prof. Dr. Ulrich Matter"
date: "04/03/2019"
output:   
  ioslides_presentation:
    css: ../../style/ioslides.css
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```


# Updates

## Learning resources

- Additional links on StudyNet
- Additional hints in GitHub-repo README

## Make use of your own machine

- From now on: more hands-on exercises
- Have Git/GitHub set up
- (Clone the course repo)

## Survey regarding Part II

- *Who has experience with Python?*

# Recap Week 2

## Four strategies of programing with Big Data

1. Use the right building blocks (R-packages)
2. Exploit/avoid R's idiosyncrasies
3. (Connect to lower-level language, such as C)
4. Use an alternative statistical procedure/estimator


## OLS in R

```{r}
beta_ols <- 
     function(X, y) {
          
          # compute cross products and inverse
          XXi <- solve(crossprod(X,X))
          Xy <- crossprod(X, y) 
          
          return( XXi  %*% Xy )
     }
```

## Monte Carlo study

- Parameters and pseudo data

```{r}
# set parameter values
n <- 10000000
p <- 4 

# Generate sample based on Monte Carlo
# generate a design matrix (~ our 'dataset') with four variables and 10000 observations
X <- matrix(rnorm(n*p, mean = 10), ncol = p)
# add column for intercept
X <- cbind(rep(1, n), X)

```

## Monte Carlo study

- Model and model output

```{r}
# MC model
y <- 2 + 1.5*X[,2] + 4*X[,3] - 3.5*X[,4] + 0.5*X[,5] + rnorm(n)

```


## Monte Carlo study

- Performance of OLS

```{r}
# apply the ols estimator
beta_ols(X, y)
```


## The Uluru algorithm as an alternative to OLS

Following @dhillon_2013, we compute $\hat{\beta}_{Uluru}$:

$$\hat{\beta}_{Uluru}=\hat{\beta}_{FS} + \hat{\beta}_{correct}$$, where
$$\hat{\beta}_{FS} = (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1}\mathbf{X}_{subs}^{\intercal}\mathbf{y}_{subs}$$, and
$$\hat{\beta}_{correct}= \frac{n_{subs}}{n_{rem}} \cdot (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1} \mathbf{X}_{rem}^{\intercal}\mathbf{R}_{rem}$$, and
$$\mathbf{R}_{rem} = \mathbf{Y}_{rem} - \mathbf{X}_{rem}  \cdot \hat{\beta}_{FS}$$.

## The Uluru algorithm as an alternative to OLS

- Key idea: Compute $(\mathbf{X}^\intercal\mathbf{X})^{-1}$ only on a sub-sample ($X_{subs}$, etc.)
- If the sample is large enough (which is the case in a Big Data context), the result is approximately the same.

## Uluru algorithm in R (simplified)

```{r}

# simple version of the Uluru algorithm
beta_uluru <-
     function(X_subs, y_subs, X_rem, y_rem) {
          
          # compute beta_fs (this is simply OLS applied to the subsample)
          XXi_subs <- solve(crossprod(X_subs, X_subs))
          Xy_subs <- crossprod(X_subs, y_subs)
          b_fs <- XXi_subs  %*% Xy_subs
          
          # compute \mathbf{R}_{rem}
          R_rem <- y_rem - X_rem %*% b_fs
          
          # compute \hat{\beta}_{correct}
          b_correct <- (nrow(X_subs)/(nrow(X_rem))) * XXi_subs %*% crossprod(X_rem, R_rem)

          # beta uluru       
          return(b_fs + b_correct)
     }

```


## Uluru algorithm in R (simplified)

Test it with the same input as above:

```{r}
# set size of subsample
n_subs <- 1000
# select subsample and remainder
n_obs <- nrow(X)
X_subs <- X[1L:n_subs,]
y_subs <- y[1L:n_subs]
X_rem <- X[(n_subs+1L):n_obs,]
y_rem <- y[(n_subs+1L):n_obs]

# apply the uluru estimator
beta_uluru(X_subs, y_subs, X_rem, y_rem)
```


## Uluru algorithm: Monte Carlo study

```{r}
# define subsamples
n_subs_sizes <- seq(from = 1000, to = 500000, by=10000)
n_runs <- length(n_subs_sizes)
# compute uluru result, stop time
mc_results <- rep(NA, n_runs)
mc_times <- rep(NA, n_runs)
for (i in 1:n_runs) {
     # set size of subsample
     n_subs <- n_subs_sizes[i]
     # select subsample and remainder
     n_obs <- nrow(X)
     X_subs <- X[1L:n_subs,]
     y_subs <- y[1L:n_subs]
     X_rem <- X[(n_subs+1L):n_obs,]
     y_rem <- y[(n_subs+1L):n_obs]
     
     mc_results[i] <- beta_uluru(X_subs, y_subs, X_rem, y_rem)[2] # the first element is the intercept
     mc_times[i] <- system.time(beta_uluru(X_subs, y_subs, X_rem, y_rem))[3]
     
}

```


## Uluru algorithm: Monte Carlo study

```{r}

# compute ols results and ols time
ols_time <- system.time(beta_ols(X, y))
ols_res <- beta_ols(X, y)[2]

```



## Uluru algorithm: Monte Carlo study

- Visualize comparison with OLS.

```{r}
# load packages
library(ggplot2)

# prepare data to plot
plotdata <- data.frame(beta1 = mc_results,
                       time_elapsed = mc_times,
                       subs_size = n_subs_sizes)
```

## Uluru algorithm: Monte Carlo study

1. Computation time.

```{r}
ggplot(plotdata, aes(x = subs_size, y = time_elapsed)) +
     geom_point(color="darkgreen") + 
     geom_hline(yintercept = ols_time[3],
                color = "red", 
                size = 1) +
     theme_minimal() +
     ylab("Time elapsed") +
     xlab("Subsample size")
```

## Uluru algorithm: Monte Carlo study

2. Precision

```{r}
ggplot(plotdata, aes(x = subs_size, y = beta1)) +
     geom_hline(yintercept = ols_res,
                color = "red", 
                size = 1) +
     geom_point(color="darkgreen") + 

     theme_minimal() +
     ylab("Estimated coefficient") +
     xlab("Subsample size")

```


# Computation and Memory

## Components of a standard computing environment

&nbsp;

```{r components, echo=FALSE, out.width = "80%", fig.align='center', fig.cap= "Basic components of a standard computing environment. Figure by @murrell_2009 (Figure 9.1, licensed under [CC BY-NC-SA 3.0 NZ](https://creativecommons.org/licenses/by-nc-sa/3.0/nz/)).", purl=FALSE}
include_graphics("../img/03_script-hardware.png")
```

## Central Processing Unit

&nbsp;

```{r cpu, echo=FALSE, out.width = "10%", fig.align='center', purl=FALSE}
include_graphics("../img/03_script-cpu.png")
```

```{r cpu2, echo=FALSE, out.width = "30%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu.jpg")
```




## Random Access Memory

&nbsp;

```{r ram, echo=FALSE, out.width = "10%", fig.align='center', purl=FALSE}
include_graphics("../img/03_script-ram.png")
```

```{r ram2, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../img/03_ram.jpg")
```





## Mass storage: hard drive

&nbsp;

```{r harddrive, echo=FALSE, out.width = "10%", fig.align='center', purl=FALSE}
include_graphics("../img/03_script-massstorage.png")
```

```{r harddrive2, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../img/03_harddrive.jpg")
```

## Now, what is Big Data (Analytics)?

- *Big Data Analytics*: The amount of data to be analyzed is not compatible with the standard usage of one or several of the computing environment's hardware components (the components fail or work very inefficiently). 

## Now, what is Big Data (Analytics)?

- *Big Data Analytics*: The amount of data to be analyzed is not compatible with the standard usage of one or several of the computing environment's hardware components (the components fail or work very inefficiently). 
- Need to understand how to *make best use of the available resources*, given a specific data analysis task.


## Now, what is Big Data (Analytics)?

- *Big Data Analytics*: The amount of data to be analyzed is not compatible with the standard usage of one or several of the computing environment's hardware components (the components fail or work very inefficiently). 
- Need to understand how to *make best use of the available resources*, given a specific data analysis task.
     - CPU: Parallel processing (use all cores available)
     - RAM: Efficient memory allocation and usage
     - RAM + Mass Storage: Virtual memory, efficient swapping
     

## Already using all components most efficiently?

- *Scale up ('vertical scaling')*
- *Scale out ('horizontal scaling')*



# Units of information/data storage

---



```{r blackbox, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu_blackbox.png")
```


## The binary system

Microprocessors can only represent two signs (states): 

 - 'Off' = `0`
 - 'On' = `1`

```{r onoff, echo=FALSE, out.width = "10%", fig.align='center', purl=FALSE}
include_graphics("../img/03_on_off.png")
```


## The binary system

- Only two signs: `0`, `1`.
- Base 2.
- Columns: $2^0=1$, $2^1=2$, $2^2=4$, and so forth.



## The binary system

What is the decimal number *139* in the binary counting frame?


## The binary system

What is the decimal number *139* in the binary counting frame?
 
 - Solution:
 
$$(1 \times 2^7) + (1 \times 2^3) + (1 \times 2^1) + (1 \times 2^0) = 139.$$



## The binary system

What is the decimal number *139* in the binary counting frame?
 
 - Solution:
 
$$(1 \times 2^7) + (1 \times 2^3) + (1 \times 2^1) + (1 \times 2^0) = 139.$$

  - More precisely:
  
$$(1 \times 2^7) + (0 \times 2^6) +  (0 \times 2^5) +  (0 \times 2^4) + (1 \times 2^3)\\ + (0 \times 2^2) + (1 \times 2^1) +  (1 \times 2^0)  = 139.$$

  - That is, the number `139` in the decimal system corresponds to `10001011` in the binary system.


## Units of information storage

  - Smallest unit (a `0` or a `1`): *bit* (from *bi*nary dig*it*; abbrev. 'b').
  - *Byte* (1 byte = 8 bits; abbrev. 'B')
    - For example, `10001011` (`139`)
  - (4 bytes (or 32 bits) are called a *word*.)
  
## Units of information storage

```{r bitbyteword, echo=FALSE, out.width = "80%", fig.align='center', fig.cap= "Bit, Byte, Word. Figure by @murrell_2009 (licensed under [CC BY-NC-SA 3.0 NZ](https://creativecommons.org/licenses/by-nc-sa/3.0/nz/))", purl=FALSE}
include_graphics("../img/03_store-bitbyteword.png")
```

## Units of information storage

Bigger units for storage capacity usually build on bytes:

 - $1 \text{ kilobyte (KB)} = 1000^{1}   \approx 2^{10}   \text{ bytes}$
 - $1 \text{ megabyte (MB)} = 1000^{2}   \approx 2^{20}   \text{ bytes}$
 - $1 \text{ gigabyte (GB)} = 1000^{3}   \approx 2^{30}   \text{ bytes}$
 - $1 \text{ terabyte (TB)} = 1000^{4}   \approx 2^{40}   \text{ bytes}$
 - $1 \text{ petabyte (PB)} = 1000^{5}   \approx 2^{50}   \text{ bytes}$
 - $1 \text{ exabyte (EB)} = 1000^{6}    \approx 2^{60}   \text{ bytes}$
 - $1 \text{ zettabyte (ZB)} = 1000^{7}  \approx 2^{70}   \text{ bytes}$

$$1 ZB = 1000000000000000000000\text{ bytes} = 1 \text{ billion terabytes} = 1 \text{ trillion gigabytes}.$$

## Information storage and data types

- Binary code can be interpreted in different ways
- As text, number, etc.
- Depending on the data type of a string of symbols such as `139`, more or less bytes are needed to represent it.

## Example Size of R objects

```{r}
object.size("139")
object.size(139)
```


# Resource Allocation in R

## R-tools to investigate performance/resource allocation

package | function | purpose
-------- | ---------- | ---------------------------------------------
`utils`  | `object.size()` | Provides an estimate of the memory that is being used to store an R object.
`pryr`   | `object_size()` | Works similarly to `object.size()`, but counts more accurately and includes the size of environments.
`pryr` | `compare_size()` | Makes it easy to compare the output of object_size and object.size.
`pryr` | `mem_used()`     | Returns the total amount of memory (in megabytes) currently used by R.
`pryr` | `mem_change()`   | Shows the change in memory (in megabytes) before and after running code.
`base`   | `system.time()` | Returns CPU (and other) times that an R expression used.
`microbenchmark` | `microbenchmark()` | Highly accurate timing of R expression evaluation.
`profvis`| `profvis()`   | Profiles an R expression and visualizes the profiling data (usage of memory, time elapsed, etc.)


## Case study: Parallel processing


We start with importing the data into R.
```{r}
stopdata <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/carData/MplsStops.csv")
```

## Case study: Parallel processing

First, let's remove observations with missing entries (`NA`) and code our main explanatory variable and the dependent variable.

```{r}
# remove incomplete obs
stopdata <- na.omit(stopdata)
# code dependent var
stopdata$vsearch <- 0
stopdata$vsearch[stopdata$vehicleSearch=="YES"] <- 1
# code explanatory var
stopdata$white <- 0
stopdata$white[stopdata$race=="White"] <- 1
```


## Case study: Parallel processing

We specify our baseline model as follows. 

```{r}
model <- vsearch ~ white + factor(policePrecinct)
```

## Case study: Parallel processing

And estimate the linear probability model via OLS (the `lm` function).

```{r}
fit <- lm(model, stopdata)
summary(fit)
```

## Case study: Parallel processing

Compute bootstrap clustered standard errors.

```{r message=FALSE}
# load packages
library(data.table)
# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)
# draw bootstrap samples, estimate model for each sample
for (i in 1:B) {
     
     # draw sample of precincts (cluster level)
     precincts_i <- sample(precincts, size = 5, replace = TRUE)
     # get observations
     bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
     bs_i <- rbindlist(bs_i)
     
     # estimate model and record coefficients
     boot_coefs[i,] <- coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
}
```

## Case study: Parallel processing

Finally, let's compute $SE_{boot}$.

```{r}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```


## Case study: Parallel processing

Parallel implementation...

```{r message=FALSE}
# install.packages("doSNOW", "parallel")
# load packages for parallel processing
library(doSNOW)

# get the number of cores available
ncores <- parallel::detectCores()
# set cores for parallel processing
ctemp <- makeCluster(ncores) # 
registerDoSNOW(ctemp)


# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)

# bootstrapping in parallel
boot_coefs <- 
     foreach(i = 1:B, .combine = rbind, .packages="data.table") %dopar% {
          
          # draw sample of precincts (cluster level)
          precincts_i <- sample(precincts, size = 5, replace = TRUE)
          # get observations
          bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
          bs_i <- rbindlist(bs_i)
          
          # estimate model and record coefficients
          coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
      
     }


# be a good citizen and stop the snow clusters
stopCluster(cl = ctemp)


```

## Case study: Parallel processing

As a last step, we compute again $SE_{boot}$.

```{r}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```



## Case study: Memory allocation


```{r eval = FALSE}
###########################################################
# Big Data Statistics: Flights data import and preparation
#
# U. Matter, January 2019
###########################################################

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"

# DATA IMPORT ----------------
flights <- read.csv(DATA_PATH)

# DATA PREPARATION --------
flights <- flights[,-1:-3]



```

## Case study: Memory allocation

Inspect the memory usage.

```{r}

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"
# load packages
library(pryr) 


# check how much memory is used by R (overall)
mem_used()

# check the change in memory due to each step

# DATA IMPORT ----------------
mem_change(flights <- read.csv(DATA_PATH))

# DATA PREPARATION --------
flights <- flights[,-1:-3]

# check how much memory is used by R now
mem_used()
```


## Case study: Memory allocation

'Collect the garbage'...

```{r}
gc()
```


## Case study: Memory allocation

Alternative approach (via memory mapping).

```{r}
# load packages
library(data.table)

# DATA IMPORT ----------------
flights <- fread(DATA_PATH, verbose = TRUE)

```


## Case study: Memory allocation

Alternative approach (via memory mapping).


```{r}

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"
# load packages
library(pryr) 
library(data.table)

# housekeeping
flights <- NULL
gc()

# check the change in memory due to each step

# DATA IMPORT ----------------
mem_change(flights <- fread(DATA_PATH))



```

## Beyond memory

<!-- What if all RAM of our computer is not enough to store all the data we want to analyze? -->

<!-- ```{r vm, echo=FALSE, out.width = "60%", fig.align='center', fig.cap= "Virtual memory. Figure by Ehamberg ([CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/))", purl=FALSE} -->
<!-- include_graphics("../img/03_virtualmemory.png") -->
<!-- ``` -->


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


