---
title: "Big Data Analytics"
subtitle: 'Lecture 3:<br>Computation and Memory Part II'
author: "Prof. Dr. Ulrich Matter"
date: "05/03/2020"
output:
  ioslides_presentation:
    css: ../../style/ioslides.css
    template: ../../style/nologo_template.html
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---

```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```


# Updates

## Schedule update {.smaller}

 1. Introduction: Big Data, Data Economy. Walkowiak (2016): Chapter 1.
 2. Computation and Memory in Applied Econometrics.
 3. *Computation and Memory in Applied Econometrics II.*
 4. *Advanced R Programming. Wickham (2019): Chapters 2, 3, 17,23, 24.*
 5. Import, Cleaning and Transformation of Big Data. Walkowiak (2016): Chapter 3: p. 74‑118.
 6. Aggregation and Visualization. Walkowiak (2016): Chapter 3: p. 118‑127; Wickham et al.(2015); Schwabish (2014).
 7. Data Storage, Databases Interaction with R. Walkowiak (2016): Chapter 5.
 8. *Cloud Computing: Introduction/Overview, Distributed Systems, Walkowiak (2016): Chapter 4.*
 9. Applied Econometrics with Spark; Machine Learning and GPUs.
 10. Project Presentations (7 May, 2020; 08:15-10:00; Room 23-103).
 11. Project Presentations; Q&A.

## Build groups for group examination

- Teams of 2-3.
- See *Canvas Announcement*.
- All team members must have a GitHub account.

## Group examination: take-home exercises

- Analysis of (big) dataset in R.
- Report  in R Markdown.
- Conceptual questions.
- Collaborate, hand-in, feedback via GitHub.

## Group projects:

- A simple empirical research question.
- A large (>2GB) data set (of your choice).
  - Get inspired [here](https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0) and [here](https://registry.opendata.aws/)
- Implement analysis in R.
- Present results in 6-7 minutes.
  - R-markdown (ioslides/shiny) or R presentation.
- Q&A, Feedback

## Group projects:

- Send short *disposition* to ulrich.matter@unisg.ch by end of March.
   - Data set (short description/link)
   - Research question
   - Idea for analysis (statistical approach)
- Have slides ready 


## Goals for today

1. Understand basics of how to control resource allocation in R.
2. Know the basics of parallel computing in R.
3. Know the basics of efficient memory allocation and virtual memory (in data analytics context).


# Recap of Week 2


## Components of a computing environment
 

```{r cpu2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu.jpg")
```

```{r ram2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_ram.jpg")
```

```{r harddrive2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_harddrive.jpg")
```


## Components of a computing environment

<center> *Why should we care?* </center>


## Big Data (Analytics)

- Find an efficient (fast) statistical procedure. (Uluru vs OLS example)
- Need to understand how to *make best use of the available resources*, given a specific data analysis task.
     - CPU: Parallel processing (use all cores available)
     - RAM: Efficient memory allocation and usage
     - RAM + Mass Storage: Virtual memory, efficient swapping


# Computation and Memory (Part II)

# Efficient Use of Resources

## 1) Parallel processing: CPU/core

- A CPU on any modern computer has several *cores*. 
- The OS usually assigns automatically which tasks/processes should run on which core.
- We can explicitly instruct the computer to dedicate $N$ cores to a specific computational task: *parallel processing*.

&nbsp;

```{r cpu3, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu.jpg")
```

## 2) Memory allocation: RAM

- Standard computation procedures happen *in-memory*: data needs to be loaded into RAM.
- Default lower-level procedures to *allocate memory* might not be optimal for large data sets.
- We can explicitly use *faster* memory allocation procedures for a specific big data task.

```{r ram3, echo=FALSE, out.width = "30%", fig.align='center', purl=FALSE}
include_graphics("../img/03_ram.jpg")
```

## 3) Beyond RAM: virtual memory

 - What if we run out of RAM?
 - The OS deals with this by using part of the hard disk as *virtual memory*.
 - By explicitly instructing the computer how to use *virtual memory for specific big data tasks*, we can speed things up.

<!-- ```{r vm, echo=FALSE, out.width = "60%", fig.align='center', fig.cap= "Virtual memory. Figure by Ehamberg ([CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/))", purl=FALSE} -->
<!-- include_graphics("../img/03_virtualmemory.png") -->
<!-- ``` -->


## Case study: Parallel processing

We start with importing the data into R.
```{r}
url <- "https://vincentarelbundock.github.io/Rdatasets/csv/carData/MplsStops.csv"
stopdata <- read.csv(url)
```

## Case study: Parallel processing

First, let's remove observations with missing entries (`NA`) and code our main explanatory variable and the dependent variable.

```{r}
# remove incomplete obs
stopdata <- na.omit(stopdata)
# code dependent var
stopdata$vsearch <- 0
stopdata$vsearch[stopdata$vehicleSearch=="YES"] <- 1
# code explanatory var
stopdata$white <- 0
stopdata$white[stopdata$race=="White"] <- 1
```


## Case study: Parallel processing

We specify our baseline model as follows. 

```{r}
model <- vsearch ~ white + factor(policePrecinct)
```

## Case study: Parallel processing

And estimate the linear probability model via OLS (the `lm` function).

```{r}
fit <- lm(model, stopdata)
summary(fit)
```

## Case study: Parallel processing

Compute bootstrap clustered standard errors.

```{r message=FALSE}
# load packages
library(data.table)
# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)
# draw bootstrap samples, estimate model for each sample
for (i in 1:B) {
     
     # draw sample of precincts (cluster level)
     precincts_i <- sample(precincts, size = 5, replace = TRUE)
     # get observations
     bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
     bs_i <- rbindlist(bs_i)
     
     # estimate model and record coefficients
     boot_coefs[i,] <- coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
}
```

## Case study: Parallel processing

Finally, let's compute $SE_{boot}$.

```{r}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```


## Case study: Parallel processing

Parallel implementation...

```{r message=FALSE}
# install.packages("doSNOW", "parallel")
# load packages for parallel processing
library(doSNOW)
# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# get the number of cores available
ncores <- parallel::detectCores()
# set cores for parallel processing
ctemp <- makeCluster(ncores) # 
registerDoSNOW(ctemp)


# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)

# bootstrapping in parallel
boot_coefs <- 
     foreach(i = 1:B, .combine = rbind, .packages="data.table") %dopar% {
          
          # draw sample of precincts (cluster level)
          precincts_i <- sample(precincts, size = 5, replace = TRUE)
          # get observations
          bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
          bs_i <- rbindlist(bs_i)
          
          # estimate model and record coefficients
          coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
      
     }


# be a good citizen and stop the snow clusters
stopCluster(cl = ctemp)


```

## Case study: Parallel processing

As a last step, we compute again $SE_{boot}$.

```{r}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```



## Case study: Memory allocation


```{r eval = FALSE}
###########################################################
# Big Data Statistics: Flights data import and preparation
#
# U. Matter, January 2019
###########################################################

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"

# DATA IMPORT ----------------
flights <- read.csv(DATA_PATH)

# DATA PREPARATION --------
flights <- flights[,-1:-3]



```

## Case study: Memory allocation

Inspect the memory usage.

```{r message=FALSE, warning=FALSE}

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"
# load packages
library(pryr) 


# check how much memory is used by R (overall)
mem_used()

# check the change in memory due to each step

# DATA IMPORT ----------------
mem_change(flights <- read.csv(DATA_PATH))

# DATA PREPARATION --------
flights <- flights[,-1:-3]

# check how much memory is used by R now
mem_used()
```


## Case study: Memory allocation

'Collect the garbage'...

```{r}
gc()
```


## Case study: Memory allocation

Alternative approach (via memory mapping).

```{r}
# load packages
library(data.table)

# DATA IMPORT ----------------
flights <- fread(DATA_PATH, verbose = TRUE)

```


## Case study: Memory allocation

Alternative approach (via memory mapping).


```{r}

# SET UP -----------------

# fix variables
DATA_PATH <- "../data/flights.csv"
# load packages
library(pryr) 
library(data.table)

# housekeeping
flights <- NULL
gc()

# check the change in memory due to each step

# DATA IMPORT ----------------
mem_change(flights <- fread(DATA_PATH))



```


## Insight from analyzing methods conceptually

- Methods for big data analytics come with an *'overhead'*
     - Additional 'preparatory' steps.
     - Only faster than traditional methods if data set has a certain size!

## Insight from analyzing methods conceptually

- Methods for big data analytics come with an *'overhead'*
     - Additional 'preparatory' steps.
     - Only faster than traditional methods if data set has a certain size!
- Examples: 
     - Parallel processing: Distribute data/task, combine afterwards.
     - `fread`: Memory maps data before actually reading it into RAM.
     
     


## Beyond memory

 - RAM is not sufficient to handle the amount of data to be analyzed...
 - *What to do?*
 
## Beyond memory

 - RAM is not sufficient to handle the amount of data to be analyzed...
 - *What to do?*
 - Scale up by using parts of the available Mass Storage (hard-disk) as *virtual memory*
 


## Out-of-memory strategies

- Chunked data files on disk
- Memory-mapped files and shared memory

## Out-of-memory strategies

- Chunked data files on disk: `ff`-package
- Memory-mapped files and shared memory: `bigmemory`-package




## Chunking data with the `ff`-package

Preparations 
```{r message=FALSE}

# SET UP --------------

# install.packages(c("ff", "ffbase"))
# load packages
library(ff)
library(ffbase)
library(pryr)

# create directory for ff chunks, and assign directory to ff 
system("mkdir ffdf")
options(fftempdir = "ffdf")

```


## Chunking data with the `ff`-package

Import data, inspect change in RAM.

```{r echo=FALSE, message=FALSE, warning=FALSE}
gc()
```


```{r}
mem_change(
flights <- 
     read.table.ffdf(file="../data/flights.csv",
                     sep=",",
                     VERBOSE=TRUE,
                     header=TRUE,
                     next.rows=100000,
                     colClasses=NA)
)
```


## Chunking data with the `ff`-package

Inspect file chunks on disk and data structure in R environment.

```{r}
# show the files in the directory keeping the chunks
list.files("ffdf")

# investigate the structure of the object created in the R environment
summary(flights)
```



## Memory mapping with `bigmemory`

Preparations

```{r message=FALSE}

# SET UP ----------------

# load packages
library(bigmemory)
library(biganalytics)
```



## Memory mapping with `bigmemory`

Import data, inspect change in RAM.

```{r}
# import the data
flights <- read.big.matrix("../data/flights.csv",
                     type="integer",
                     header=TRUE,
                     backingfile="flights.bin",
                     descriptorfile="flights.desc")
```


## Memory mapping with `bigmemory`

Inspect the imported data.

```{r}
summary(flights)
```


## Memory mapping with `bigmemory`

Inspect the object loaded into the R environment.

```{r}
flights
```


## Memory mapping with `bigmemory`

- `backingfile`: The cache for the imported file (holds the raw data on disk).
- `descriptorfile`: Metadata describing the imported data set (also on disk).


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

First, import a large data set without a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

# import data and check memory used
mem_change(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

flights1 
```




## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Second, import the same data set with a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

# import data and check memory used
mem_change(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

flights2
```


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Third, re-import the same data set with a backing-file.

```{r}
# remove the loaded file
rm(flights2)

# 'load' it via the backing-file
system.time(flights2 <- attach.big.matrix("flights2.desc"))

flights2

```




## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>
