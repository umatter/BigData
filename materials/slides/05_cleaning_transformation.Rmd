---
title: "Big Data Analytics"
subtitle: 'Lecture 5:<br>Cleaning and Transformation of Big Data'
author: "Prof. Dr. Ulrich Matter"
date: "25/03/2021"
output:
  ioslides_presentation:
    css: ../../style/ioslides.css
    template: ../../style/nologo_template.html
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---



```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```




 
# Recap 

## Bindings basics

- Objects/values do not have names but _names have values_!
- Objects have a 'memory address'/identifiers.

```{r}
x <- c(1, 2, 3)
```

```{r binding1, echo=FALSE, out.width = "25%", purl=FALSE, fig.align='left', purl=FALSE}
include_graphics("../img/binding-1.png")
```


## Copy-on-modify

- If we modify values in a vector, actual 'copying' is necessary (depending on the data structure of the object...).

```{r binding3, echo=FALSE, out.width = "25%", purl=FALSE, fig.align='left', purl=FALSE}
include_graphics("../img/binding-3.png")
```

## Data structures and modify-in-place

```{r list3, echo=FALSE, out.width = "35%", fig.align='left', fig.cap= "Figure by @wickham_2019 (licensed under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)).", purl=FALSE}
include_graphics("../img/l-modify-2.png")
```


## Improving performance

- Bottleneck(s) identified, what now?
- See previous examples for typical problems in a data analytics context.
- Vast variety of potential bottlenecks. Hard to give general advice.

## Programming with Big Data

1. Which basic (already implemented) R functions are more or less suitable as building blocks for the program?
2. How can we exploit/avoid some of R's lower-level characteristics in order to implement efficient functions?
3. Is there a need to interface with a lower-level programming language in order to speed up the code? (advanced topic)

+ Independent of *how* we write a statistical procedure in R (or in any other language, for that matter), is there an *alternative statistical procedure/algorithm* that is faster but delivers approximately the same result.


## Issues to keep in mind

- Vectorization.
- Memory: avoid copying, pre-allocate memory.
- Use built in primitive (C) functions (caution: not always faster, if aim is precision).
- Existing solutions: load additional packages (`read.csv()` vs. `data.table::fread()`).
  - Focus of what follows in this course (approach taken in @walkowiak_2016).

## Procedural view and further reading

- Consider Hadley's advice: @wickham_2019: Chapter 24
- Experienced coder? Have a look at [R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf)
- Further reading after this course: [The Art of R Programming](http://heather.cs.ucdavis.edu/~matloff/132/NSPpart.pdf)


## Goals for today

1. Know basic strategies for out-of-memory operations in R.
2. Know basic tools for local big data cleaning and transformation in R.
3. Understand (in simple terms) how these tools work.
4. (Recap of virtual memory concept)

# Virtual Memory

## Virtual memory

- Operating system allocates part of mass storage device (hard-disk) as *virtual memory*. 
- Process/application uses up too much RAM, OS starts *swapping* data between RAM and virtual memory.
- Processes slow down due to swapping.
- Default (OS) usage of virtual memory concept is not necessarily optimized for data analysis tasks.

## Virtual memory


```{r vm1, echo=FALSE, out.width = "35%", fig.align='center', purl=FALSE}
include_graphics("../img/03_virtualmemory_dark.png")
```


## Virtual memory: example (linux)

```{r vm2, echo=FALSE, out.width = "95%", fig.align='center', purl=FALSE}
include_graphics("../img/05_virtual_memory_linux.gif")
```

## 'Out-of-memory' strategies

- Use virtual memory idea for specific data analytics tasks.
- Two approaches:
  - *Chunked data files on disk*: partition large data set, map and store chunks of raw data on disk. Keep mapping in RAM. (`ff`-package)
  - *Memory mapped files and shared memory*: virtual memory is explicitly allocated for one or several specific data analytics tasks (different processes can access the same memory segment). (`bigmemory`-package)


## Chunking data with the `ff`-package

Preparations 
```{r message=FALSE}

# SET UP --------------

# install.packages(c("ff", "ffbase"))
# load packages
library(ff)
library(ffbase)
library(pryr)

# create directory for ff chunks, and assign directory to ff 
system("mkdir ffdf")
options(fftempdir = "ffdf")

```


## Chunking data with the `ff`-package

Import data, inspect change in RAM.

```{r echo=FALSE, message=FALSE, warning=FALSE}
gc()
```


```{r}
mem_change(
flights <- 
     read.table.ffdf(file="../data/flights.csv",
                     sep=",",
                     VERBOSE=TRUE,
                     header=TRUE,
                     next.rows=100000,
                     colClasses=NA)
)
```


## Chunking data with the `ff`-package

Inspect file chunks on disk and data structure in R environment.

```{r}
# show the files in the directory keeping the chunks
list.files("ffdf")

# investigate the structure of the object created in the R environment
summary(flights)
```



## Memory mapping with `bigmemory`

Preparations

```{r message=FALSE}

# SET UP ----------------

# load packages
library(bigmemory)
library(biganalytics)
```



## Memory mapping with `bigmemory`

Import data, inspect change in RAM.

```{r}
# import the data
flights <- read.big.matrix("../data/flights.csv",
                     type="integer",
                     header=TRUE,
                     backingfile="flights.bin",
                     descriptorfile="flights.desc")
```


## Memory mapping with `bigmemory`

Inspect the imported data.

```{r}
summary(flights)
```


## Memory mapping with `bigmemory`

Inspect the object loaded into the R environment.

```{r}
flights
```


## Memory mapping with `bigmemory`

- `backingfile`: The cache for the imported file (holds the raw data on disk).
- `descriptorfile`: Metadata describing the imported data set (also on disk).


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

First, import a large data set without a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

# import data and check memory used
mem_change(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

flights1 
```




## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Second, import the same data set with a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

# import data and check memory used
mem_change(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

flights2
```


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Third, re-import the same data set with a backing-file.

```{r}
# remove the loaded file
rm(flights2)

# 'load' it via the backing-file
system.time(flights2 <- attach.big.matrix("flights2.desc"))

flights2

```



# Cleaning and Transformation 

---

```{r pipeline, echo=FALSE, out.width = "95%", fig.align='center', purl=FALSE}
include_graphics("../img/05_data_science_pipeline.png")
```



## Typical tasks (independent of data set size)

- Normalize/standardize.
- Code additional variables (indicators, strings to categorical, etc.).
- Remove, add covariates.
- Merge data sets.
- Set data types.

## Typical workflow

1. Import raw data.
2. Clean/transform.
3. Store for analysis.
     - Write to file.
     - Write to database.
     
## Bottlenecks

- RAM:
     - Raw data does not fit into memory.
     - Transformations enlarge RAM allocation (copying).
- Mass Storage: Reading/Writing
- CPU: Parsing (data types)



# Data Preparation with `ff`

## Set up

The following examples are based on @walkowiak_2016, Chapter 3.

```{r warning=FALSE, message=FALSE}

## SET UP ------------------------

#Set working directory to the data and airline_id files.
# setwd("materials/code_book/B05396_Ch03_Code")
system("mkdir ffdf")
options(fftempdir = "ffdf")

# load packages
library(ff)
library(ffbase)
library(pryr)

# fix vars
FLIGHTS_DATA <- "../code_book/B05396_Ch03_Code/flights_sep_oct15.txt"
AIRLINES_DATA <- "../code_book/B05396_Ch03_Code/airline_id.csv"

```

## Data import

```{r}

# DATA IMPORT ------------------

# 1. Upload flights_sep_oct15.txt and airline_id.csv files from flat files. 

system.time(flights.ff <- read.table.ffdf(file=FLIGHTS_DATA,
                                          sep=",",
                                          VERBOSE=TRUE,
                                          header=TRUE,
                                          next.rows=100000,
                                          colClasses=NA))

airlines.ff <- read.csv.ffdf(file= AIRLINES_DATA,
                             VERBOSE=TRUE,
                             header=TRUE,
                             next.rows=100000,
                             colClasses=NA)
# check memory used
mem_used()

```


## Comparison with `read.table`

```{r}

##Using read.table()
system.time(flights.table <- read.table(FLIGHTS_DATA, 
                                        sep=",",
                                        header=TRUE))

gc()

system.time(airlines.table <- read.csv(AIRLINES_DATA,
                                       header = TRUE))


# check memory used
mem_used()

```


## Inspect imported files

```{r}
# 2. Inspect the ffdf objects.
## For flights.ff object:
class(flights.ff)
dim(flights.ff)
## For airlines.ff object:
class(airlines.ff)
dim(airlines.ff)

```

## Data cleaning and transformation

Goal: merge airline data to flights data


```{r}
# step 1: 
## Rename "Code" variable from airlines.ff to "AIRLINE_ID" and "Description" into "AIRLINE_NM".
names(airlines.ff) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.ff)
str(airlines.ff[1:20,])
```


## Data cleaning and transformation

Goal: merge airline data to flights data

```{r}
# merge of ffdf objects
mem_change(flights.data.ff <- merge.ffdf(flights.ff, airlines.ff, by="AIRLINE_ID"))

class(flights.data.ff)
dim(flights.data.ff)
dimnames(flights.data.ff)
```

## Inspect difference to in-memory operation

```{r}
##For flights.table:
names(airlines.table) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.table)
str(airlines.table[1:20,])

# check memory usage of merge in RAM 
mem_change(flights.data.table <- merge(flights.table,
                                       airlines.table,
                                       by="AIRLINE_ID"))
```



## Subsetting

```{r}
mem_used()

# Subset the ffdf object flights.data.ff:
subs1.ff <- subset.ffdf(flights.data.ff, CANCELLED == 1, 
                        select = c(FL_DATE, AIRLINE_ID, 
                                   ORIGIN_CITY_NAME,
                                   ORIGIN_STATE_NM,
                                   DEST_CITY_NAME,
                                   DEST_STATE_NM,
                                   CANCELLATION_CODE))

dim(subs1.ff)
mem_used()

```


## Save to ffdf-files
(For further processing with `ff`)

```{r}
# Save a newly created ffdf object to a data file:

save.ffdf(subs1.ff, overwrite = TRUE) #7 files (one for each column) created in the ffdb directory

```


## Load ffdf-files

```{r}
# Loading previously saved ffdf files:
rm(subs1.ff)
gc()
load.ffdf("ffdb")
str(subs1.ff)
dim(subs1.ff)
dimnames(subs1.ff)
```

## Export to CSV

```{r message=FALSE}
#  Export subs1.ff into CSV and TXT files:
write.csv.ffdf(subs1.ff, "subset1.csv")

```




## References

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>